#!/usr/bin/perl

#
# defaults
#

use Parse::AccessLogEntry;
use strict;
use Getopt::GUI::Long;
use XML::Simple;
use Data::Dumper;
use File::Temp qw(tempfile);
use IO::File;

our $VERSION = "0.51";

#
# Option parsing
#
my %opts = (d => '/var/log/httpd');

Getopt::GUI::Long::Configure(qw(display_help no_ignore_case
				capture_output no_gui allow_zero));
GetOptions(\%opts,
	   ["c|config-file=s",   "XML Config File to use"],
	   ["s|site=s",          "Only look for data for website name STRING"],
	   ["d|log-directory=s", "Directory containing the log files"],
	   ["S|log-suffix=s",    "Adds a file suffix to log files searched for"],
	   ["I|include-path=s",  "Adds search path components for include files"],
	   ["GUI:separator",     "Debugging"],
	   ["debug",             "Debugging output"],
	   ["D|dump-config",     "Dump config being used for each site"],
	   ["w|write-config=s",  "Writes the resulting combined config to a particular file"],
	   ["GUI:otherargs_text", "[LOG_FILES]"],
	  ) || exit 1;

#
# Configuration File
#
# read in the .xml config file with special <include> processing
#
my $xmlfile = $opts{'c'} || $ENV{'HOME'} . "/.httpsum/config.xml";

my ($tmpxmlh, $tmpxmlfile);
if ($opts{'w'}) {
    $tmpxmlfile = $opts{'w'};
    $tmpxmlh = new IO::File;
    $tmpxmlh->open("> $tmpxmlfile") || die "can't open $opts{'w'} for writing";
} else {
    ($tmpxmlh, $tmpxmlfile) = tempfile();
}

DEBUG("creating temporary XML file from input files");
read_xml_file($tmpxmlh, $xmlfile);
$tmpxmlh->close();

DEBUG("reading XML file");
my $config = XMLin($tmpxmlfile,
		   ForceArray => [qw(ignorefile agent site ignorehost
				     transformreferer transformfile
				     ignorereferer ignoreuseragent)]);
unlink($tmpxmlfile) if (!$opts{'w'});

my $parser = Parse::AccessLogEntry::new();
my %results;

# figure out what sites to look for
my @sites = split(/,\s*/,$opts{'s'});
@sites = keys(%{$config->{'sites'}{'site'}}) if ($#sites == -1);

site:
foreach my $site (@sites) {
    my $sitedata = get_config($config, $site);

    print "\n----- $site -----\n\n";

    print Dumper($sitedata) if ($opts{'D'});

    # open each needed log file
    my @files;
    if ($#ARGV == -1) {
	my $files = $sitedata->{'file'};
	$files = $sitedata->{'directory'} . "/" . $files if ($files !~ /\//);
	@files = glob($files);
    } else {
	@files = @ARGV;
    }

    # output data goes in the results hash
    my %results;

    # a list of the bots that searched the site
    my %bots;

    # a count of the number of bots
    my $robots;

    foreach my $file (@files) {
	DEBUG("processing $file");
	if (! -f $file) {
	    print STDERR "** failed to open file $file\n";
	    next site;
	}

	open(I, $file);
      line:
	while (<I>) {

	    # process the line and skip if it's a HEAD request
	    my $parts = $parser->parse($_);
	    next if ($parts->{'rtype'} eq 'HEAD');

	    foreach my $map (keys(%{$sitedata->{'agent'}})) {
		if ($parts->{'agent'} =~ /$map/) {
		    $parts->{'agent'} = $sitedata->{'agent'}{$map}{'content'};
		    if ($sitedata->{'agent'}{$map}{'bot'}) {
			$bots{$parts->{'agent'}}++;
			next line;
		    }
		    last;
		}
	    }

	    # ignore things that look at the robots.txt file
	    # XXX: should be configurable; skip "U" agent
	    if ($parts->{'file'} =~ /robots.txt/) {
		$results{$parts->{'file'}}{'robot'}++;
		$results{$parts->{'file'}}{'count'}++;
# XXX
# 		push @useragentignores, $parts->{'agent'}
# 		  if (!exists($bots{$parts->{'agent'}}));

		print "** possible bot:  $parts->{'agent'}\n";

		$bots{$parts->{'agent'}}++;
		$robots++;
		next;
	    }

	    #
	    # ignore lines
	    #

	    # process the list of files to ignore
	    foreach my $fileig (@{$sitedata->{'ignorefile'}}) {
		next line if ($parts->{'file'} =~ /$fileig/);
	    }

	    # ignore things based on referals
	    foreach my $refer (@{$sitedata->{'ignorereferer'}}) {
		next line if ($parts->{'refer'} =~ /$refer/);
	    }

	    foreach my $host (@{$sitedata->{'ignorehost'}}) {
		next line if ($parts->{'host'} =~ /$host/);
	    }

	    foreach my $useragent (@{$sitedata->{'ignoreuseragent'}}) {
		next line if ($parts->{'agent'} eq $useragent);
	    }

	    #
	    # transformations
	    #
	    foreach my $transform (keys(%{$sitedata->{'transformreferer'}})) {
		$parts->{'refer'} =~ s/$transform/eval "\"$sitedata->{'transformreferer'}{$transform}{'content'}\""/e;
	    }

	    foreach my $transform (keys(%{$sitedata->{'transformfile'}})) {
		$parts->{'file'} =~ s/$transform/eval "\"$sitedata->{'transformfile'}{$transform}{'content'}\""/e;
	    }

	    # finally classify the results
	    $results{$parts->{'file'}}{$parts->{'refer'}}++;
	    $results{$parts->{'file'}}{'count'}++;
	}
    }

    print "Bot hits:\n";
    foreach my $bot (keys(%bots)) {
	$bot =~ s/[^;]+; *//;
	$bot =~ s/;.*//;
	printf("%6d %s\n", $bots{$bot}, $bot);
    }

    print "\nHits:\n";
    foreach my $file (sort { $results{$a}{'count'} <=> $results{$b}{'count'} } keys(%results)) {
	printf "%6d %-70.70s\n", $results{$file}{'count'}, $file;
	foreach my $referer (sort { $results{$file}{$a} <=> $results{$file}{$b} } keys(%{$results{$file}})) {
	    next if ($referer eq 'count');
	    next if ($referer eq '-');
	    printf "  %6d %-70.70s\n",$results{$file}{$referer}, $referer;
	}
    }
}


sub get_config {
    my ($xmldata, $site) = @_;

    my $sitedata = $xmldata->{'sites'}{'site'}{$site};

    # ARRAYs
    # copy each value from a set of config tokens into the array
    foreach my $key (qw(ignorefile ignorereferer ignorehost)) {
	foreach my $value (@{$xmldata->{'global'}{$key}}) {
	    push @{$sitedata->{$key}}, $value;
	}
    }

    # SCALARs
    # copy each value from a set of config tokens into the array
    foreach my $key (qw(directory file)) {
	$sitedata->{$key} = $xmldata->{'global'}{$key}
	  if (!exists($sitedata->{$key}));
	$sitedata->{$key} =~ s/\%{site}/$site/g;
    }

    # NAME = VALUE
    # copy each value and each name into the config array
    foreach my $key (qw(agent transformfile transformreferer)) {
	next if (!exists($xmldata->{'global'}{$key}));
	foreach my $name (keys(%{$xmldata->{'global'}{$key}})) {
	    if (!exists($sitedata->{$key}{$name})) {
		$sitedata->{$key}{$name} = $xmldata->{'global'}{$key}{$name};
	    }
	}
    }

    # supply a few defaults
    $sitedata->{'file'} = "$site.log" if (!exists($sitedata->{'file'}));
    $sitedata->{'directory'} = $opts{'d'} if (!exists($sitedata->{'directory'}));

    $sitedata->{'file'} .= $opts{'S'} if (defined($opts{'S'}));

    return $sitedata;
}

sub read_xml_file {
    my ($tmpxmlh, $file) = @_;
    my $fileh = new IO::File;

    # locate the file
    if (! -f $file) {
	my @searchpath = (".", split(",", $opts{'I'}),
			  $ENV{'HOME'} . "/.httpsum",
			  "/usr/share/httpsum/include-modules",
			  "/usr/local/share/httpsum/include-modules");
	my $found = 0;
	foreach my $path (@searchpath) {
	    if (-f "$path/$file") {
		$file = "$path/$file";
		$found = 1;
		last;
	    }
	}
	if (!$found) {
	    print STDERR "*** failed to locate include file '$file'; continuing anyway\n";
	    return;
	}
    }

    DEBUG("  reading $file");
    $fileh->open("< $file");
    while (<$fileh>) {
	if (/<include>\s*(.*)<\/include>/) {
	    my $file = $1;
	    chomp($file);
	    read_xml_file($tmpxmlh, $file);
	} elsif (/<include\s+src=\"(.*)\"\s*\/>/){
	    read_xml_file($tmpxmlh, $1);
	}
	print $tmpxmlh $_;
    }
    $fileh->close();
}

sub DEBUG {
    if ($opts{'debug'}) {
	print @_,"\n";
    }
}


=pod

=head1 NAME

httpsum - Summarize apache log files

=head1 SYNOPSIS

httpsum -d /path/to/logfiles [OPTIONS] [LOG_FILES...]

=head1 DESCRIPTION

B<httpsum> strives to analyze log files and give you just the results
you care about.  Too many log file analyzers exist but few let you
perform proper filtering and aggregation down to the level of just the
page-hits for the pages you care about.  B<httpsum> tries to fix that
by simply dumping a summary table of the results after various filters
and transformations have been applied.

=head1 OPTIONS

=over

=item -d PATH

Specifies the directory to look in for logfiles.

=item -S suffix

Specifies the filename suffix to require.  Useful for looking for log
entries for a specific date, if the logs are rotated daily.  EG:

  httpsum -S .`date -d yesterday +%Y-%m-%d`

=item -c XML_CONFIG_FILE

This is fundamental configuration file used to decide what sites are
being analyzed, how to interpret the logs, etc.  See the
B<XML_CONFIG_FILE> section below for complete details on the format of
this file.

If not specified then the default $HOME/.httpsum/config.xml file will
be used.

=item -s SITE

Assume all the logs read are for a single SITE.  Normally the
XML_CONFIG_FILE can identify multiple sites to report information
about, but this option allows the output to be limited to a single
site.

=item -I INCLUDE_PATH

Specifies an optional include path to use when using the <include...>
directive of the XML_CONFIG_FILE.

=item --debug

Extra verbose debugging about exactly what B<httpsum> is doing.

=item -D

Dumps the configuration file per-site that is being used to analyze
the log files for that site.  This reports both the global and site
specific options as finally combined.

=back

=head1 XML_CONFIG_FILE

The XML_CONFIG_FILE is a configuration file that dictates how
reporting should be done and for what sites.  If no file is specified
via the command line then B<httpsum> will look in I<~/.httpsum.xml>.

The contents of the file will take the following high-level format:

  <httpsum>
    <global>
      <!-- Global options that apply to all sites -->
    </global>
    <sites>
      <!-- Sites to analyze and site-specific options>
    </sites>
  </httpsum>

Any directive below can appear in either the site-specific section or
in the global section.  Global options will map to each site, but
site-specific options will only apply to that individual site.

=over

=item <file>FILESE<lt>/file>

Specifies the files to read; possibly with wild-card matching.  A
special %{site} keyword can be used when placed in the global section
to add the site name into the file pattern.

Example:

  <file>/var/log/httpd/%{site}/access.log.*</file>

Note that this directive is ignored if log files are specified on the
command line instead.

=item <ignorehost>HOSTE<lt>/ignorehost>

Ignores hosts of a particular address.  EG,

  <ignorehost>127.0.0.1</ignorehost>

Will not analyze log file lines generated from requests from the
localhost.

=item <ignorereferrer>REFERERE<lt>/ignorereferrer>

If you wish to ignore accesses that were referred to from particular
location this token will let you do that.  This is handy for only
analyzing incoming requests that came from a remote or bookmarked
location, for example.  By ignoring the site name itself it'll ensure
that first incoming connections are examined.

=item <ignorefile>FILEE<lt>/ignorefile>

Ignores requests to FILES (really path components).  This is useful
for ignoring common files that provide no useful data, like CSS files
or image directories.  The I<FILE> specifier is actually a regular
expression so expressions like "\.css$" and "^/2010/.*/foo$" are valid
expressions.

=item <agent name="MATCH_REGEXP" bot="1|0">NAMEE<lt>/agent>

If a given agent name matches the I<MATCH_REGEXP> regular expression
then it will be translated into NAME when analyzed.  This is most
useful when the I<bot> attribute is set to 1 as the web crawling bot
hit will not be counted as a normal hit and will simply be summarized
in the bot specific output section.

=item <transformfile name="REGEXP">REPLACEMENTE<lt>/transformfile>

Replaces a URL with an alternative version.  This is designed to make
longer URL strings easier to read.

For example, take the complex gallery2 URL that doesn't make much sense to look at quickly and the following line will transform the URL into a much more simple to read "Image: NUMBER" lien:

    <transformfile name=".*core.Download.*g2_itemId=(\d+).*">Image: $1</transformfile>

=item <transformreferer name="REGEXP">REPLACEMENTE<lt>/transformreferer>

Similar to transformfile, but applies to referer strings.

For example,

    <transformreferer name=".*facebook.*share.*">facebook: share</transformreferer>

Will translate any "share" item from a facebook referer into a simple
"facebook: share" string so that you simply receive a count of how
many times this page was hit by someone "sharing" it on facebook.

=item <include src="FILE" />

This includes the contents of another file into the one currently
being processed.  The I<FILE> may refer to a complete file or
path-name.  If it can't be immediately found then files in the
following search paths will be checked for:

  .
  -I switch paths if given
  $HOME/.httpsum
  /usr/share/httpsum/include-modules
  /usr/local/share/httpsum/include-modules"

Some include files that may be of interest are distributed with the
httpsum application; see further below for details.

Note that this is an easy-to-use include statement that is not fully
XML-legal (if you wish to use a XML-legal syntax, please see the use
of XML "entities" in XML language documentation).

=back

=head1 HTTPSUM DISTRIBUTED INCLUDE FILES

The following include files are distributed with httpsum:

=over

=item agents.xml

A file containing many of the common bot/web-crawlers.  It is highly
recommended you include this file in the <global> section of your
configuration.

=item transforms.xml

A list of transformations converting complex URLs into easy-to-read
outputs.  For example, search engines are converted from their full
URL to strings like "engine: word1+word2...".

=item type-wordpress.xml

Contains useful exclude patterns for including in wordpress sites.

=item type-gallery2.xml

Contains useful exclude patterns for including in gallery2 sites.
Also transforms certain URL patterns into easy-to-read results like
"Image: NUMBER".

=back

=head1 EXAMPLE

Consider the following configuration file:

  <httpsum>
    <global>
      <include src="agents.xml" />
    </global>
    <sites>
      <site name="capturedonearth.com">
        <include>type-gallery2.xml</include>
      </site>
    </sites>
  </httpsum>

Then the following shows example output when run as follows on the
log-files from the http://capturedonearth.com/ website:

 # httpsum -c config.xml capturedonearth.com/access.log.2010-05-1*

 ----- capturedonearth.com -----

 Bot hits:
      8 Ask Jeeves
      3 SurveyBot
    548 dotnetdotcom
     21 Twiceler
   1663 MSN
   ...

 Hits:
      1 Item: 6379
      1 core: DownloadItem - 6831
        1 Item: 81
      1 core: DownloadItem - 6647
        1 Item: 8545
      ...
     28 Item: 8545
        1 http://twitter.com/
        1 Item: 8549
        1 Item: 8545
        1 http://twitter.com/NevadaWolf/geocachers
        6 http://touch.facebook.com/
     62 slideshow: DownloadPicLens
        1 Item: 7128
        1 slideshow: Slideshow - 35
        ...
        2 slideshow: Slideshow - 6638
        3 slideshow: Slideshow - 6363
        8 http://capturedonearth.com/main.php
       10 Item: 8545
       13 Item: 8549

The indented lines are the referring site.  EG, picture number 8545
was referred to 6 times by http://touch.facebook.com/.

=head1 AUTHOR

Wes Hardaker < hardaker AT users DT sourceforge TOD net >

=head1 COPYRIGHT and LICENSE

Copyright 2009-2010, Wes Hardaker.  All rights reserved. 

httpsum is free software; you can redistribute it and/or modify it
under the same terms as Perl itself.

=cut

