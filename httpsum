#!/usr/bin/perl

#
# defaults
#

use Parse::AccessLogEntry;
use strict;
use Getopt::GUI::Long;
use XML::Simple;
use Data::Dumper;
use File::Temp qw(tempfile);
use IO::File;

#
# Option parsing
#
my %opts = (d => '/var/log/httpd');

Getopt::GUI::Long::Configure(qw(display_help no_ignore_case
				capture_output no_gui allow_zero));
GetOptions(\%opts,
	   ["c|config-file=s",   "XML Config File to use"],
	   ["s|site=s",          "Only look for data for website name STRING"],
	   ["d|log-directory=s", "Directory containing the log files"],
	   ["S|log-suffix=s",    "Adds a file suffix to log files searched for"],
	   ["I|include-path=s",  "Adds search path components for include files"],
	   ["GUI:separator",     "Debugging"],
	   ["debug",             "Debugging output"],
	   ["D|dump-config",     "Dump config being used for each site"],
	   ["GUI:otherargs_text", "[LOG_FILES]"],
	  ) || exit 1;

#
# Configuration File
#
# read in the .xml config file with special <include> processing
#
my $xmlfile = $opts{'c'} || $ENV{'HOME'} . "/.httpsum.xml";

my ($tmpxmlh, $tmpxmlfile) = tempfile();

DEBUG("creating temporary XML file from input files");
read_xml_file($tmpxmlh, $xmlfile);
$tmpxmlh->close();

DEBUG("reading XML file");
my $config = XMLin($tmpxmlfile,
		   ForceArray => [qw(ignorefile agent site
				     transformreferer transformfile
				     ignorereferer ignoreuseragent)]);
unlink($tmpxmlfile);

my $parser = Parse::AccessLogEntry::new();
my %results;

# figure out what sites to look for
my @sites = split(/,\s*/,$opts{'s'});
@sites = keys(%{$config->{'sites'}{'site'}}) if ($#sites == -1);

site:
foreach my $site (@sites) {
    my $sitedata = get_config($config, $site);

    print "\n----- $site -----\n\n";

    print Dumper($sitedata) if ($opts{'D'});

    # open each needed log file
    my @files;
    if ($#ARGV == -1) {
	my $files = $sitedata->{'file'};
	$files = $sitedata->{'directory'} . "/" . $files if ($files !~ /\//);
	@files = glob($files);
    } else {
	@files = @ARGV;
    }

    # output data goes in the results hash
    my %results;

    # a list of the bots that searched the site
    my %bots;

    # a count of the number of bots
    my $robots;

    foreach my $file (@files) {
	DEBUG("processing $file");
	if (! -f $file) {
	    print STDERR "** failed to open file $file\n";
	    next site;
	}

	open(I, $file);
      line:
	while (<I>) {

	    # process the line and skip if it's a HEAD request
	    my $parts = $parser->parse($_);
	    next if ($parts->{'rtype'} eq 'HEAD');

	    foreach my $map (keys(%{$sitedata->{'agent'}})) {
		if ($parts->{'agent'} =~ /$map/) {
		    $parts->{'agent'} = $sitedata->{'agent'}{$map}{'content'};
		    if ($sitedata->{'agent'}{$map}{'bot'}) {
			$bots{$parts->{'agent'}}++;
			next line;
		    }
		    last;
		}
	    }

	    # ignore things that look at the robots.txt file
	    # XXX: should be configurable; skip "U" agent
	    if ($parts->{'file'} =~ /robots.txt/) {
		$results{$parts->{'file'}}{'robot'}++;
		$results{$parts->{'file'}}{'count'}++;
# XXX
# 		push @useragentignores, $parts->{'agent'}
# 		  if (!exists($bots{$parts->{'agent'}}));

		print "** possible bot:  $parts->{'agent'}\n";

		$bots{$parts->{'agent'}}++;
		$robots++;
		next;
	    }

	    #
	    # ignore lines
	    #

	    # process the list of files to ignore
	    foreach my $fileig (@{$sitedata->{'ignorefile'}}) {
		next line if ($parts->{'file'} =~ /$fileig/);
	    }

	    # ignore things based on referals
	    foreach my $refer (@{$sitedata->{'ignorereferer'}}) {
		next line if ($parts->{'refer'} =~ /$refer/);
	    }

	    foreach my $host (@{$sitedata->{'ignorehost'}}) {
		next line if ($parts->{'host'} =~ /$host/);
	    }

	    foreach my $useragent (@{$sitedata->{'ignoreuseragent'}}) {
		next line if ($parts->{'agent'} eq $useragent);
	    }

	    #
	    # transformations
	    #
	    foreach my $transform (keys(%{$sitedata->{'transformreferer'}})) {
		$parts->{'refer'} =~ s/$transform/eval "\"$sitedata->{'transformreferer'}{$transform}{'content'}\""/e;
	    }

	    foreach my $transform (keys(%{$sitedata->{'transformfile'}})) {
		$parts->{'file'} =~ s/$transform/eval "\"$sitedata->{'transformfile'}{$transform}{'content'}\""/e;
	    }

	    # finally classify the results
	    $results{$parts->{'file'}}{$parts->{'refer'}}++;
	    $results{$parts->{'file'}}{'count'}++;
	}
    }

    print "Bot hits:\n";
    foreach my $bot (keys(%bots)) {
	$bot =~ s/[^;]+; *//;
	$bot =~ s/;.*//;
	printf("%6d %s\n", $bots{$bot}, $bot);
    }

    print "\nHits:\n";
    foreach my $file (sort { $results{$a}{'count'} <=> $results{$b}{'count'} } keys(%results)) {
	printf "%6d %-70.70s\n", $results{$file}{'count'}, $file;
	foreach my $referer (sort { $results{$file}{$a} <=> $results{$file}{$b} } keys(%{$results{$file}})) {
	    next if ($referer eq 'count');
	    next if ($referer eq '-');
	    printf "  %6d %-70.70s\n",$results{$file}{$referer}, $referer;
	}
    }
}


sub get_config {
    my ($xmldata, $site) = @_;

    my $sitedata = $xmldata->{'sites'}{'site'}{$site};

    # ARRAYs
    # copy each value from a set of config tokens into the array
    foreach my $key (qw(ignorefile)) {
	foreach my $value (@{$xmldata->{'global'}{$key}}) {
	    push @{$sitedata->{$key}}, $value;
	}
    }

    # SCALARs
    # copy each value from a set of config tokens into the array
    foreach my $key (qw(directory file)) {
	$sitedata->{$key} = $xmldata->{'global'}{$key}
	  if (!exists($sitedata->{$key}));
	$sitedata->{$key} =~ s/\%{site}/$site/g;
    }

    # NAME = VALUE
    # copy each value and each name into the config array
    foreach my $key (qw(agent transformreferer)) {
	foreach my $name (keys(%{$xmldata->{'global'}{$key}})) {
	    if (!exists($sitedata->{$key}{$name})) {
		$sitedata->{$key}{$name} = $xmldata->{'global'}{$key}{$name};
	    }
	}
    }

    # supply a few defaults
    $sitedata->{'file'} = "$site.log" if (!exists($sitedata->{'file'}));
    $sitedata->{'directory'} = $opts{'d'} if (!exists($sitedata->{'directory'}));

    $sitedata->{'file'} .= $opts{'S'} if (defined($opts{'S'}));

    return $sitedata;
}

sub read_xml_file {
    my ($tmpxmlh, $file) = @_;
    my $fileh = new IO::File;

    # locate the file
    if (! -f $file) {
	my @searchpath = (".", split(",", $opts{'I'}),
			  $ENV{'HOME'} . "/.httpsum",
			  "/usr/share/httpsum/include-modules",
			  "/usr/local/share/httpsum/include-modules");
	my $found = 0;
	foreach my $path (@searchpath) {
	    if (-f "$path/$file") {
		$file = "$path/$file";
		$found = 1;
		last;
	    }
	}
	if (!$found) {
	    print STDERR "*** failed to locate include file '$file'; continuing anyway\n";
	    return;
	}
    }

    DEBUG("  reading $file");
    $fileh->open("< $file");
    while (<$fileh>) {
	if (/<include>\s*(.*)<\/include>/) {
	    my $file = $1;
	    chomp($file);
	    read_xml_file($tmpxmlh, $file);
	} elsif (/<include\s+src=\"(.*)\"\s*\/>/){
	    read_xml_file($tmpxmlh, $1);
	}
	print $tmpxmlh $_;
    }
    $fileh->close();
}

sub DEBUG {
    if ($opts{'debug'}) {
	print @_,"\n";
    }
}


=pod

=head1 NAME

httpsum - Summarize apache log files

=head1 SYNOPSIS

httpsum -d /path/to/logfiles [OPTIONS] [LOG_FILES...]

=head1 OPTIONS

=over

=item -d PATH

Specifies the directory to look in for logfiles.

=item -S suffix

Specifies the filename suffix to require.  Useful for looking for log
entries for a specific date, if the logs are rotated daily.  EG:

  httpsum -S .`date -d yesterday +%Y-%m-%d`

=item -c XML_CONFIG_FILE

This is fundamental configuration file used to decide what sites are
being analyized, how to interpret the logs, etc.  See the
B<XML_CONFIG_FILE> section below for complete details on the format of
this file.

=item -s SITE

Assume all the logs read are for a single SITE.  Normally the
XML_CONFIG_FILE can identify multiple sites to report information
about, but this option allows the output to be limited to a single
site.

=item -I INCLUDE_PATH

Specifies an optional include path to use when using the <include...>
directive of the XML_CONFIG_FILE.

=item --debug

Extra verbose debugging about exactly what B<httpsum> is doing.

=item -D

Dumps the configuration file per-site that is being used to analyize
the log files for that site.  This reports both the global and site
specific options as finally combined.

=back

=head1 XML_CONFIG_FILE

The XML_CONFIG_FILE is a configuration file that dictates how
reporting should be done and for what sites.  If no file is specified
via the command line then B<httpsum> will look in I<~/.httpsum.xml>.

The contents of the file will take the following high-level format:

  <httpsum>
    <global>
      <!-- Global options that apply to all sites -->
    </global>
    <sites>
      <!-- Sites to analyize and site-specific options>
    </sites>
  </httpsum>

Any directive below can appear in either the site-specific section or
in the global section.  Global options will map to each site, but
site-specific options will only apply to that individual site.

=over

=item <file>FILES</file>

Specifies the files to read; possibly with wild-card matching.  A
special %{site} keyword can be used when placed in the global section
to add the site name into the file pattern.

Example:

  <file>/var/log/httpd/%{site}/access.log.*</file>

=item <ignorehost>HOST</ignorehost>

Ignores hosts of a particular address.  EG,

  <ignorehost>127.0.0.1</ignorehost>

Will not analyize log file lines generated from requests from the
localhost.

=item <ignorereferrer>REFERER</ignorereferrer>

If you wish to ignore accesses that were refered to from particular
location this token will let you do that.  This is handy for only
analyzizing incoming requests that came from a remote or bookmarkde
location, for example.  By ignoring the site name itself it'll ensure
that first incoming connections are examined.

=item <ignorefile>FILE</ignorefile>

Ignores requests to FILES (really path components).  This is useful
for ignoring common files that provide no useful data, like CSS files
or image directories.  The I<FILE> specifier is actually a regular
expression so expressions like "\.css$" and "^/2010/.*/foo$" are valid
expressions.

=item <agent name="MATCH_REGEXP" bot="1|0">NAME</agent>

If a given agent name matches the I<MATCH_REGEXP> regular expression
then it will be translated into NAME when analyized.  This is most
useful when the I<bot> attribute is set to 1 as the web crawling bot
hit will not be counted as a normal hit and will simply be summarized
in the bot specific output section.

=back

=cut

